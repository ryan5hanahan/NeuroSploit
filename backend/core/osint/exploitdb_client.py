"""
ExploitDB Client — searches the ExploitDB CSV archive for known exploits.

Downloads the ExploitDB CSV from the official GitLab mirror on first use,
caches on disk at /tmp/sploitai_exploitdb/ with 24-hour refresh.
Parses into an in-memory dict keyed by CVE ID for fast lookups.
"""

import csv
import io
import logging
import os
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import aiohttp

from backend.core.osint.base_client import OSINTClient

logger = logging.getLogger(__name__)

EXPLOITDB_CSV_URL = (
    "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"
)
CACHE_DIR = Path("/tmp/sploitai_exploitdb")
CACHE_FILE = CACHE_DIR / "files_exploits.csv"
CACHE_MAX_AGE = 86400  # 24 hours


class ExploitDBClient(OSINTClient):
    SERVICE_NAME = "exploitdb"
    RATE_LIMIT_PER_SECOND = 5.0  # local lookups are fast
    CACHE_TTL_SECONDS = 86400

    def __init__(self, api_key: str = ""):
        super().__init__(api_key)
        # CVE -> list of exploit dicts
        self._cve_index: Dict[str, List[Dict[str, Any]]] = {}
        # Full list for keyword search
        self._all_exploits: List[Dict[str, Any]] = []
        self._loaded = False

    @property
    def enabled(self) -> bool:
        return True  # No API key required

    async def enrich_target(self, domain: str, session: aiohttp.ClientSession) -> Dict[str, Any]:
        """Not used for ExploitDB — satisfies ABC."""
        return {"source": "exploitdb"}

    # ------------------------------------------------------------------
    # Public search methods
    # ------------------------------------------------------------------

    async def search_by_cve(
        self,
        cve_id: str,
        session: aiohttp.ClientSession,
    ) -> List[Dict[str, Any]]:
        """Return ExploitDB entries matching a CVE ID."""
        await self._ensure_loaded(session)
        return self._cve_index.get(cve_id, [])

    async def search_by_keyword(
        self,
        keyword: str,
        session: aiohttp.ClientSession,
        max_results: int = 5,
    ) -> List[Dict[str, Any]]:
        """Substring match on exploit titles."""
        await self._ensure_loaded(session)
        kw_lower = keyword.lower()
        matches = []
        for entry in self._all_exploits:
            if kw_lower in entry.get("title", "").lower():
                matches.append(entry)
                if len(matches) >= max_results:
                    break
        return matches

    # ------------------------------------------------------------------
    # CSV download / parse
    # ------------------------------------------------------------------

    async def _ensure_loaded(self, session: aiohttp.ClientSession) -> None:
        """Download CSV if needed and parse into memory."""
        if self._loaded:
            return

        csv_text = await self._get_csv(session)
        if csv_text:
            self._parse_csv(csv_text)

        self._loaded = True

    async def _get_csv(self, session: aiohttp.ClientSession) -> Optional[str]:
        """Return CSV text from disk cache or download."""
        CACHE_DIR.mkdir(parents=True, exist_ok=True)

        # Use disk cache if fresh
        if CACHE_FILE.exists():
            age = time.time() - CACHE_FILE.stat().st_mtime
            if age < CACHE_MAX_AGE:
                try:
                    return CACHE_FILE.read_text(errors="replace")
                except Exception as e:
                    logger.warning(f"ExploitDB cache read failed: {e}")

        # Download fresh copy
        try:
            async with session.get(
                EXPLOITDB_CSV_URL,
                timeout=aiohttp.ClientTimeout(total=120),
                ssl=False,
            ) as resp:
                if resp.status != 200:
                    logger.warning(f"ExploitDB CSV download returned {resp.status}")
                    # Fall back to stale cache if available
                    if CACHE_FILE.exists():
                        return CACHE_FILE.read_text(errors="replace")
                    return None
                text = await resp.text()
                CACHE_FILE.write_text(text)
                logger.info(f"ExploitDB CSV downloaded ({len(text)} bytes)")
                return text
        except Exception as e:
            logger.warning(f"ExploitDB CSV download failed: {e}")
            if CACHE_FILE.exists():
                return CACHE_FILE.read_text(errors="replace")
            return None

    def _parse_csv(self, csv_text: str) -> None:
        """Parse ExploitDB CSV into in-memory indices."""
        reader = csv.DictReader(io.StringIO(csv_text))
        cve_index: Dict[str, List[Dict[str, Any]]] = {}
        all_exploits: List[Dict[str, Any]] = []

        for row in reader:
            try:
                edb_id = row.get("id", "").strip()
                title = row.get("description", "").strip()
                platform = row.get("platform", "").strip()
                etype = row.get("type", "").strip()
                codes_raw = row.get("codes", "")

                entry = {
                    "edb_id": edb_id,
                    "title": title,
                    "platform": platform,
                    "type": etype,
                }
                all_exploits.append(entry)

                # Parse CVE codes (semicolon-separated, e.g. "CVE-2024-1234;OSVDB-12345")
                if codes_raw:
                    for code in codes_raw.split(";"):
                        code = code.strip()
                        if code.startswith("CVE-"):
                            cve_index.setdefault(code, []).append(entry)
            except Exception:
                continue

        self._cve_index = cve_index
        self._all_exploits = all_exploits
        logger.info(
            f"ExploitDB loaded: {len(all_exploits)} exploits, "
            f"{len(cve_index)} CVE mappings"
        )
